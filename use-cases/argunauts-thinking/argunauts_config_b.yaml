# Custom configuration for tool use Chain of Thought enhancement
llm:
  # Provider selection: "vllm" or "api-endpoint"
  provider: "api-endpoint"

vllm:
  api_base: "http://localhost:8000/v1"
  model: "unsloth/Meta-Llama-3.3-70B-Instruct"
  max_retries: 3
  retry_delay: 1.0

# API endpoint configuration
api-endpoint:
  api_base: null # Optional base URL for API endpoint (null for default API)
  api_key: null # API key for API endpoint or compatible service (can use env var instead)
  model: "kit.gpt-oss-120b" # Default model to use
  max_retries: 3 # Number of retries for API calls
  retry_delay: 1.0 # Initial delay between retries (seconds)
  sleep_time: 0.5 # Small delay in seconds between batches to avoid rate limits

generation:
  temperature: 0.3 # Lower temperature for more consistent reasoning
  top_p: 0.95
  max_tokens: 8192 # Allow for longer outputs to accommodate CoT reasoning

# The most important part - our custom Chain of Thought prompt
prompts:
  cot_enhancement: |
    You are an expert editor of synthetic argumentation conversations from the `DebateLabKIT/argunauts-thinking` dataset.
    Your job is to improve alignment between user instructions and assistant behaviour while preserving the logical content of the assistant answers.

    You will receive one or more conversations serialized as JSON.
    Each conversation is an array of message objects with keys such as `role`, `content`, `name`, `thinking`, `tool_calls`, and `tools`.

    STRICT INVARIANTS (MUST FOLLOW):
    - Do NOT add or remove any messages.
    - Do NOT add or remove any keys in any message.
    - Do NOT change any `role`, `name`, `tool_calls`, or `tools` field.
    - For all messages with `role: "assistant"`, you MUST treat the `content` field as read-only text.
      Do not modify a single character of any `assistant.content` value.
    - You MAY rewrite the `thinking` field of assistant messages.
    - For `system` and `tool` messages, you should leave all fields unchanged.

    EXPECTED TRANSFORMATION –  – Thinking-focused alignment with minimal prompt edits:
    1. Prefer to keep `user` message `content` as close as possible to the original.
       - You MAY make small clarifications, fix minor phrasing, or add short phrases when necessary so that the assistant's behaviour is not wildly surprising.
       - Avoid substantially lengthening or radically rephrasing user messages; the spirit and granularity of the original instruction should remain recognizable.
    2. Use the `thinking` field of assistant messages as the main place to realign the conversation.
       - Rewrite `assistant.thinking` to give a realistic chain of thought that:
         - Begins with 1–3 concise sentences restating, in your own words, what the user is asking for and what outcome they want.
           - Use formulations such as "The user is asking me to...", "The user faces ... and asks me to...", or "The user's goal is to...".
           - Ground this restatement in the (mostly unchanged) user prompt and do not introduce new tasks or goals that the user did not ask for.
         - Then develops a rich explanation of how the assistant arrives at its already-fixed `content` and any tool calls.
           - Start from the user’s request and outline a plausible approach (e.g. analysing the text, extracting propositions, reconstructing arguments, running rhetorical tools, adding YAML annotations), but this outline does not have to be an exhaustive numbered checklist.
           - Explicitly refer to concrete actions that appear in the assistant’s `content`, such as:
             - reconstructing arguments in Argdown (premise–conclusion structures, labels, inference rules),
             - formalizing premises and conclusions in Syntax.LATEX or similar symbolic notation,
             - using rhetorical analysis tools (for example `rhetorical_analysis_multi`) on specific propositions,
             - embedding tool outputs (e.g. tone, persuasion scores, cognitive complexity) as inline YAML annotations.
       - The chain of thought does not need to be strictly linear.
         - It may include realistic backtracking, trying alternative approaches, and short “taking stock” moments, such as:
           - "Wait, am I really capturing the user’s focus on rhetorical style here?"
           - "Let me reconsider whether this argument structure matches the narrative flow of the text."
           - "Now that I have tried plan A (purely formal reconstruction), I will instead follow the text paragraph by paragraph to get a better result."
         - These non-linear moves should remain coherent, build on each other, and clearly move towards the existing assistant `content` and tool calls.
         - You may revise or refine your initial plan inside the `thinking` (e.g. adjusting how you structure the Argdown reconstruction or when you call tools), but the overall trajectory must still be traceable from the user’s request to the final answer.
       - You may restructure or shorten overlong step lists from the original `thinking`.
         - Replace overly mechanical "Step 1–16" plans with more natural, higher-level planning plus occasional reflections, as long as the reasoning still justifies why a detailed, multi-step answer is appropriate.
         - Keep meta-cognitive commentary (e.g. backtracking notes, plan updates) purposeful and relatively brief; avoid turning the chain of thought into pure chatter.


    Additional guidelines:
    - Preserve the argumentative content, Argdown structures, and logical relationships expressed in the assistant `content` exactly as given.
    - Maintain the overall style of the dataset: precise, analytic, and focused on argument mapping and applied logic.

    Output requirements:
    - Return the full conversations in the SAME JSON structure you received, with the same number of conversations and messages.
    - Only the `user.content` fields (minimally, as described above) and `assistant.thinking` fields may change.
    - Do NOT add any commentary, explanations, or markdown outside of the JSON itself.

    BEGIN WORK NOW. Rewrite the conversation(s) according to these rules:
    {conversations}
